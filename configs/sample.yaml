mode: sample_eval
seed: 42
block_size: 4

algo:
  name: bd3lm
  backbone: dit           # dit / dimamba / hf_dit
  parameterization: subs  # subs / sedd / ar
  time_conditioning: false
  T: 128
  causal_attention: false
  dropout: 0.0
  ignore_bos: true
  cross_attn: true
  var_min: false
  sampler: semi_ar
  mdlm_loss_scale: false

# model will be merged and overridden by configs/model/<model>.yaml at runtime
model:
  length: 512
  attn_backend: sdpa       # Do not use flex during sampling

loader:
  eval_batch_size: 1

sampling:
  num_sample_batches: 1
  var_length: true
  logdir: ./sample_logs/simple_sample_
  nucleus_p: 0.95
  first_hitting: true
  top1: true
  kv_cache: false
  stop_on_eos_only: true
  entropy_stop: false

eval:
  checkpoint_path: ./checkpoints/last.ckpt
  disable_ema: false
  perplexity_batch_size: 8

data:
  tokenizer_name_or_path: vocab_V2.txt
  smiles_path: null
  wrap: false

noise:
  type: loglinear

training:
  ema: 0.9999
  antithetic_sampling: true
  sampling_eps: 1e-3
  sampling_eps_min: 1e-3
  sampling_eps_max: 1.0
